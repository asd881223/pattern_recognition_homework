{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937004fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.4\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f294f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:56:49.621766: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 15:56:49.760277: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-23 15:56:50.289740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 15:56:50.289796: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 15:56:50.289803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), \\\n",
    "(test_images, test_labels)= mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191a71a",
   "metadata": {},
   "source": [
    "## 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba50bbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb400397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  22, 139, 212, 253, 159, 138,  11,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         17, 234, 253, 252, 252, 252, 252, 136,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51,\n",
       "        209, 252, 253, 240, 183, 240, 252, 247, 184, 101,  13,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  70,\n",
       "        252, 252, 192,  37,   0, 207, 252, 253, 252, 252, 139,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 122,\n",
       "        252, 252,  63,   0,   0, 207, 252, 253, 252, 252, 160,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 123,\n",
       "        253, 253,  11,  15, 108, 233, 253, 255, 253, 236,  94,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  70,\n",
       "        252, 252, 151, 219, 252, 252, 252, 196,  92,  25,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  70,\n",
       "        252, 252, 253, 252, 252, 195, 130,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  70,\n",
       "        252, 252, 253, 235,  77,   9,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 189, 236,\n",
       "        252, 252, 253, 206,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   9,  24, 255, 249, 199,\n",
       "        116, 241, 255, 207,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  34, 174, 252, 249, 117,   0,\n",
       "          0, 209, 253, 236,  44,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  13, 203, 252, 252, 167,   0,   0,\n",
       "          0, 116, 253, 252, 152,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   9, 174, 252, 252, 168,   0,   0,   0,\n",
       "          0,  21, 253, 252, 183,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  24, 252, 252, 116,   2,   0,   0,   0,\n",
       "          0,   0, 253, 252, 246,  42,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  24, 253, 222,  25,   0,   0,   0,   0,\n",
       "          0,   0, 149, 253, 253,  46,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  24, 252, 252, 135,  26,   0,   0,   0,\n",
       "          0,  22, 253, 252, 240,  37,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  17, 196, 252, 252, 221, 185, 111, 101,\n",
       "        184, 215, 253, 240, 133,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   9, 129, 236, 252, 253, 252, 252,\n",
       "        252, 252, 253, 164,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  19, 128, 253, 252, 252,\n",
       "        252, 252,  75,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= train_images[5537]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76ae2f",
   "metadata": {},
   "source": [
    "## 查看第5537個數字是多少  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8971cff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae5822a650>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "pl.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c9945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 22139212253159138 11  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 17234253252252252252136  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 51209252253240183240252247184101 13  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 70252252192 37  0207252253252252139  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0122252252 63  0  0207252253252252160  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0123253253 11 15108233253255253236 94  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 70252252151219252252252196 92 25  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 70252252253252252195130  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 70252252253235 77  9  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 11189236252252253206  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  9 24255249199116241255207  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 34174252249117  0  0209253236 44  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 13203252252167  0  0  0116253252152  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  9174252252168  0  0  0  0 21253252183  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 24252252116  2  0  0  0  0  0253252246 42  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 24253222 25  0  0  0  0  0  0149253253 46  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 24252252135 26  0  0  0  0 22253252240 37  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 17196252252221185111101184215253240133  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  9129236252253252252252252253164  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 19128253252252252252 75  8  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        z= x[i,j]\n",
    "        print(f'{z:3d}', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa4e1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[5537]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d95e7321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= train_labels[5537]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b90f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f8facac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695a8be6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[5537]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84123f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the image data\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed3b82",
   "metadata": {},
   "source": [
    "## 一開始都沒設定，所跑出來的準確率只有25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cf670a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:57:05.723254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:57:05.752504: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-09-23 15:57:05.752531: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-09-23 15:57:05.753658: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 15:57:05.778342: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 5s 2ms/step - loss: 5.4918 - accuracy: 0.2447\n",
      " 133/1875 [=>............................] - ETA: 1s - loss: 5.4314 - accuracy: 0.2556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:57:10.487645: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 884us/step - loss: 5.3954 - accuracy: 0.2543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.395418167114258, 0.2542833387851715]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The network architecture\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "my_model = keras.Sequential([\n",
    "    layers.Dense(512),\n",
    "])\n",
    "\n",
    "# The compilation step\n",
    "my_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# \"Fitting\" the model\n",
    "\n",
    "my_model.fit(train_images, train_labels)\n",
    "my_model.evaluate(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015eff4",
   "metadata": {},
   "source": [
    "## 加一層 Dense(128, activation=\"relu\") 和 epochs=10, batch_size=28\n",
    "結果準確率更低了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3a5b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:59:51.192939: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.7980 - accuracy: 0.1173\n",
      "Epoch 2/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.5930 - accuracy: 0.1313\n",
      "Epoch 3/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.4629 - accuracy: 0.1367\n",
      "Epoch 4/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.5082 - accuracy: 0.1296\n",
      "Epoch 5/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.4283 - accuracy: 0.1407\n",
      "Epoch 6/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.4358 - accuracy: 0.1419\n",
      "Epoch 7/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.3810 - accuracy: 0.1469\n",
      "Epoch 8/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.2098 - accuracy: 0.1721\n",
      "Epoch 9/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.2406 - accuracy: 0.1737\n",
      "Epoch 10/10\n",
      "2143/2143 [==============================] - 6s 3ms/step - loss: 4.1750 - accuracy: 0.1822\n",
      "1875/1875 [==============================] - 2s 803us/step - loss: 4.3472 - accuracy: 0.1610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.347225666046143, 0.1609666645526886]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3.9874072074890137, 0.3876666724681854]\n",
    "# The network architecture\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "my_model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(128, activation=\"relu\")\n",
    "])\n",
    "\n",
    "# The compilation step\n",
    "my_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# \"Fitting\" the model\n",
    "my_model.fit(train_images, train_labels, epochs=10, batch_size=28)\n",
    "my_model.evaluate(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db54e6",
   "metadata": {},
   "source": [
    "## 把 activation 改成 softmax\n",
    "準確率發生了質的飛越提高到了99.22％！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1800616b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2143/2143 [==============================] - 6s 2ms/step - loss: 0.2115 - accuracy: 0.9384\n",
      "Epoch 2/5\n",
      "2143/2143 [==============================] - 5s 2ms/step - loss: 0.0928 - accuracy: 0.9739\n",
      "Epoch 3/5\n",
      "2143/2143 [==============================] - 5s 3ms/step - loss: 0.0667 - accuracy: 0.9816\n",
      "Epoch 4/5\n",
      "2143/2143 [==============================] - 5s 3ms/step - loss: 0.0552 - accuracy: 0.9855\n",
      "Epoch 5/5\n",
      "2143/2143 [==============================] - 5s 3ms/step - loss: 0.0453 - accuracy: 0.9887\n",
      "1875/1875 [==============================] - 2s 794us/step - loss: 0.0298 - accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.029830848798155785, 0.9921833276748657]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The network architecture\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "my_model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# The compilation step\n",
    "my_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# \"Fitting\" the model\n",
    "my_model.fit(train_images, train_labels, epochs=5, batch_size=28)\n",
    "my_model.evaluate(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e3dce",
   "metadata": {},
   "source": [
    "## 把 batch_size 提高至 128 準確率又比原本的28提升至99.87％\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3ccc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2852 - accuracy: 0.9198\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1108 - accuracy: 0.9670\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0721 - accuracy: 0.9789\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9840\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0395 - accuracy: 0.9883\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0304 - accuracy: 0.9909\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0235 - accuracy: 0.9931\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0177 - accuracy: 0.9952\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0136 - accuracy: 0.9963\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0106 - accuracy: 0.9969\n",
      "1875/1875 [==============================] - 1s 761us/step - loss: 0.0057 - accuracy: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005742198321968317, 0.9987166523933411]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The network architecture\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "my_model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# The compilation step\n",
    "my_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# \"Fitting\" the model\n",
    "my_model.fit(train_images, train_labels, epochs=10, batch_size=128)\n",
    "my_model.evaluate(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376698a",
   "metadata": {},
   "source": [
    "## 把隱藏層多加一層 keras.layers.Dense(50,  activation= 'relu') 結果與只有一層的差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91df0a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2501 - accuracy: 0.9257\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0946 - accuracy: 0.9710\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9809\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0453 - accuracy: 0.9856\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0332 - accuracy: 0.9897\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 0.9923\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9942\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0153 - accuracy: 0.9953\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0120 - accuracy: 0.9960\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0097 - accuracy: 0.9969\n",
      "1875/1875 [==============================] - 1s 742us/step - loss: 0.0042 - accuracy: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004167775623500347, 0.9987666606903076]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The network architecture\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "my_model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    keras.layers.Dense(50,  activation= 'relu'), \n",
    "    keras.layers.Dense(10,  activation= 'softmax')\n",
    "])\n",
    "\n",
    "# The compilation step\n",
    "my_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# \"Fitting\" the model\n",
    "my_model.fit(train_images, train_labels, epochs=10, batch_size=128)\n",
    "my_model.evaluate(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9cf2f",
   "metadata": {},
   "source": [
    "## 將訓練好的model拿來進行預測，測試的準確率為98.28%，相當的不錯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d61afd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 0s 701us/step - loss: 0.0764 - accuracy: 0.9828\n",
      "test_acc: 0.9828000068664551\n"
     ]
    }
   ],
   "source": [
    "# Using the model to make predictions\n",
    "\n",
    "test_digits = test_images[0:100]\n",
    "print(test_digits.shape)\n",
    "predictions = my_model.predict(test_digits)\n",
    "\n",
    "# Evaluating the model on new data\n",
    "\n",
    "test_loss, test_acc = my_model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d587c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3415c944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data representations for neural networks\n",
    "import numpy as np\n",
    "x = np.array(12)\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04da8d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectors\n",
    "x = np.array([12, 3, 6, 14, 7])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fa0debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[ 5 78  2]\n",
      " [34  0  6]\n",
      " [79  3 35]\n",
      " [ 1  7 80]\n",
      " [ 4 36  2]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "1\n",
    "# Matrices\n",
    "x = np.array([[5, 78, 2, 34, 0],\n",
    " [6, 79, 3, 35, 1],\n",
    " [7, 80, 4, 36, 2]])\n",
    "print(x.ndim)\n",
    "x1 = x.reshape(5,3)\n",
    "print(x1)\n",
    "print(x1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "879aea2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank-3 and higher-rank tensors\n",
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    " [6, 79, 3, 35, 1],\n",
    " [7, 80, 4, 36, 2]],\n",
    " [[5, 78, 2, 34, 0],\n",
    " [6, 79, 3, 35, 1],\n",
    " [7, 80, 4, 36, 2]],\n",
    " [[5, 78, 2, 34, 0],\n",
    " [6, 79, 3, 35, 1],\n",
    " [7, 80, 4, 36, 2]]])\n",
    "x.ndim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0a39551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(60000, 28, 28)\n",
      "uint8\n",
      "47040000\n",
      "47040000\n"
     ]
    }
   ],
   "source": [
    "# Key attributes\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.ndim)\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)\n",
    "print(train_images.size)\n",
    "print(train_images.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b34def81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The fourth sample in our dataset\n",
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "124dab02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[240, 253, 253, ...,   0,   0,   0],\n",
       "        [ 45, 186, 253, ...,   0,   0,   0],\n",
       "        [  0,  16,  93, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[241, 243, 234, ...,   0,   0,   0],\n",
       "        [143,  91,  28, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[253, 254, 253, ...,   0,   0,   0],\n",
       "        [ 72, 192, 254, ...,   0,   0,   0],\n",
       "        [  0,   6, 242, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,  31, 127, ...,   0,   0,   0],\n",
       "        [ 27, 218, 252, ...,   0,   0,   0],\n",
       "        [194, 253, 217, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[ 97, 254, 252, ...,   0,   0,   0],\n",
       "        [232, 181,  60, ...,   0,   0,   0],\n",
       "        [ 46,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = train_images[:, 14:, 14:]\n",
    "my_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20c55e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 49, 238, 253, ...,  93,  82,  82],\n",
       "        [ 18, 219, 253, ...,   0,   0,   0],\n",
       "        [  0,  80, 156, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ..., 253, 207,   2],\n",
       "        [  0,   0,   0, ..., 250, 182,   0],\n",
       "        [  0,   0,   0, ...,  78,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,  84, 252, 253],\n",
       "        [  0,   0,   0, ...,  96, 189, 253],\n",
       "        [  0,   0,   0, ...,  47,  79, 255],\n",
       "        ...,\n",
       "        [252, 145,   0, ..., 252, 173,   0],\n",
       "        [253, 225,   0, ..., 162,   0,   0],\n",
       "        [252, 249, 146, ...,  56,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   2, 153],\n",
       "        [  0,   0,   0, ...,   0,  27, 254],\n",
       "        [  0,   0,   0, ...,   0, 183, 254],\n",
       "        ...,\n",
       "        [  0,   0,   0, ..., 254,  57,   0],\n",
       "        [  0,   0,   0, ..., 254,  57,   0],\n",
       "        [  0,   0,   0, ..., 255,  94,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0,   0,   0, ..., 223, 159, 131],\n",
       "        [  0,   0,   0, ...,  27,   0,   0],\n",
       "        [  0,   0,  54, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ..., 173,   0,   0],\n",
       "        [  0,   0,   0, ..., 173,   0,   0],\n",
       "        [  0,   0,   0, ...,  74,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [247, 110,   0, ..., 146, 163,  63],\n",
       "        [236, 128,   0, ..., 178,  12,   0],\n",
       "        [239, 196, 169, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ..., 254, 212,  27],\n",
       "        [  0,   0,   0, ..., 218, 237, 248],\n",
       "        [  0,   0,   0, ...,   0,  92, 231],\n",
       "        ...,\n",
       "        [  0, 110, 254, ...,   0,   0,   0],\n",
       "        [131, 254, 154, ...,   0,   0,   0],\n",
       "        [209, 153,  19, ...,   0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = train_images[:, 7:-7, 7:-7]\n",
    "my_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58914ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "# The notion of data batches\n",
    "batch = train_images[:128]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95c4411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "batch = train_images[128:256]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f46dfd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "batch = train_images[128 * n:128 * (n + 1)]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206cab07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e79c4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Looking back at our first example\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c10e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e3aa570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a525373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2543 - accuracy: 0.9263\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1031 - accuracy: 0.9696\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0672 - accuracy: 0.9798\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9851\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0372 - accuracy: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae59572bf0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d14aa1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimplementing our first example from scratch in TensorFlow\n",
    "# A simple Dense class\n",
    "import tensorflow as tf\n",
    "\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "\n",
    "        b_shape = (output_size,)\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "029dbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple Sequential class\n",
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "               x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights\n",
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6080e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A batch generator\n",
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d5e88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running one training step\n",
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7c14215",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1b3640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da4ffd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full training loop\n",
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "330a7693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 4.57\n",
      "loss at batch 100: 2.23\n",
      "loss at batch 200: 2.22\n",
      "loss at batch 300: 2.08\n",
      "loss at batch 400: 2.23\n",
      "Epoch 1\n",
      "loss at batch 0: 1.91\n",
      "loss at batch 100: 1.87\n",
      "loss at batch 200: 1.84\n",
      "loss at batch 300: 1.70\n",
      "loss at batch 400: 1.83\n",
      "Epoch 2\n",
      "loss at batch 0: 1.59\n",
      "loss at batch 100: 1.57\n",
      "loss at batch 200: 1.52\n",
      "loss at batch 300: 1.42\n",
      "loss at batch 400: 1.51\n",
      "Epoch 3\n",
      "loss at batch 0: 1.33\n",
      "loss at batch 100: 1.33\n",
      "loss at batch 200: 1.25\n",
      "loss at batch 300: 1.20\n",
      "loss at batch 400: 1.28\n",
      "Epoch 4\n",
      "loss at batch 0: 1.14\n",
      "loss at batch 100: 1.14\n",
      "loss at batch 200: 1.05\n",
      "loss at batch 300: 1.04\n",
      "loss at batch 400: 1.11\n",
      "Epoch 5\n",
      "loss at batch 0: 0.99\n",
      "loss at batch 100: 1.01\n",
      "loss at batch 200: 0.91\n",
      "loss at batch 300: 0.92\n",
      "loss at batch 400: 0.99\n",
      "Epoch 6\n",
      "loss at batch 0: 0.89\n",
      "loss at batch 100: 0.90\n",
      "loss at batch 200: 0.80\n",
      "loss at batch 300: 0.83\n",
      "loss at batch 400: 0.90\n",
      "Epoch 7\n",
      "loss at batch 0: 0.80\n",
      "loss at batch 100: 0.81\n",
      "loss at batch 200: 0.72\n",
      "loss at batch 300: 0.76\n",
      "loss at batch 400: 0.84\n",
      "Epoch 8\n",
      "loss at batch 0: 0.74\n",
      "loss at batch 100: 0.75\n",
      "loss at batch 200: 0.66\n",
      "loss at batch 300: 0.71\n",
      "loss at batch 400: 0.78\n",
      "Epoch 9\n",
      "loss at batch 0: 0.69\n",
      "loss at batch 100: 0.69\n",
      "loss at batch 200: 0.61\n",
      "loss at batch 300: 0.66\n",
      "loss at batch 400: 0.74\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0db0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.81\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "import numpy as np\n",
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")\n",
    "\n",
    "# Summary\n",
    "print(train_images.itemsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97121ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
